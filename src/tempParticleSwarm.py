"""
This code runs a particle swarm optimization scheme on GOSIA to try to find the best 
minimum. PSO is random and not reproducible. For best results, run multiple instances of this
code and take the best result, then put it into a traditional fitter for fine-tuning. This
code requires GOSIA input files for OP,INTI, OP,MAP, and OP,POIN (for beam and target), with 
several specific modifications to them. The code will draw the boundary conditions from the 
OP,INTI input files (from the bounds you set in OP,EXPT). PSO relies on good selection of
boundary conditions for best results, so try to make them tight where possible (for example,
well known target matrix elements should be constrained to within a few sigma of the
literature value). In the OP,POIN input, comments are needed on the following lines in 
OP,YIEL:

NS1,NS2 - please comment this line with !NS
UPL - please comment this line with !UPL
NBRA - please comment this line with !BR
NL - please comment this line with !LT
NDL - please comment this line with !DL
NAMX - please comment this line with !ME

If you are not doing simultaneous beam/target minimization, you will also need to comment your scaling factors:

YNRM - please comment EACH of these lines with !SCL
You must comment each of those lines even for experiments that have independent scaling.

These comments are used for functions that parse the input file to be able to find your limits
and constraints. Note that this code is not set up to handle multiple YNRM or UPL values, so 
you will need to select one instance to comment and that's what the code will use.

You will also need to generate format your yields file in a very specific way. Every 
transition that will be generated by OP,POIN needs to be included, and for unobserved
transitions record the counts and uncertainty as 0 for both. The transitions also need to be
listed in the same order as they appear in OP,POIN (if you're not sure what that is, just 
run OP,POIN and check the order and then format them to match that). The parser assumes that
user-generated files, including this one, use comma separated values, not space separated.
Files that GOSIA generates it assumes the default format in which those files are generated.

There are also two changes to the code that need to be made for each user. In 
fileManagement.py, in the first line of the createSubDirectories function, you will need to 
specify the path to where the subdirectories should be created. The use of subdirectories is
so we can run multiple threads simultaneously. The other change is in myconfig.py. You will 
need to modify this file to include the names of your input files. These should not be 
absolute paths, just the file names. The path will be determined by the directory the code is
run in.

Before running this code, I recommend that you get a 'reasonable' fit from GOSIA or another
minimizer. This is so the corrected yields are relatively correct. Alternatively, you can
start with a terrible fit and use this code to iterively find better 'starting' positions
for the integration, but this takes a long time to run and it will likely be much faster to
just get a quick fit from GOSIA first.

After that, you SHOULD be able to run without any additional modifications. It's possible that
some GOSIA input files will not be read correctly by the parser if they use very different
options than I did. If that's the case, you may need to modify the code in gosiaManager.py
to get it to read your input file properly.
"""
import numpy as np
import pyswarms.backend as P
import gosiaManager
from pyswarms.backend.topology import Ring
from pyswarms.backend.topology import Star
from pyswarms.backend.handlers import VelocityHandler
from pyswarms.backend.handlers import BoundaryHandler
import threading
import os
import sys

#Takes an integer as an argument. Only effects where the name of the output file and the temp 
#directories for running GOSIA by default. Useful for running many instances in parallel.
#Ensure a different batch number is used for each instance when running simultaneously.
configFile = sys.argv[1]
batchNumber = int(sys.argv[2])

#Initializes the process of computing the particle chisq values, including dividing up the 
#particles between multiple threads.
def getSwarmChisq(positions,iteration,nThreads=1):
  particlesPerThread = int(np.ceil(len(positions)/nThreads))
  threadFirstLast = []
  folderStart = batchNumber*nThreads
  for j in range(nThreads):
    chainDirPrefix = gm.createSubDirectories(folderStart + j)
    threadFirstLast.append((particlesPerThread*j,min(particlesPerThread*(j+1),len(positions))))
  
  chisqDict = {}
  tmpChisqArray = []
  for j in range(nThreads):
    chainDir = chainDirPrefix + str(folderStart + j)
    tmpChisqArray.append(threading.Thread(target=getParticleChisq, args=[positions,iteration,threadFirstLast[j],chainDir,chisqDict], name='t%i' % j))
    tmpChisqArray[j].start()

  for j in range(nThreads):
    tmpChisqArray[j].join()
  
  chisqArray = []
  for firstLast in threadFirstLast:
    chisqArray += chisqDict[firstLast[0]]
  chisqArray = np.array(chisqArray)
  return chisqArray
  
#Each thread calls this function once. Loops over all particles assigned to that thread and
#computes the chisq values.
def getParticleChisq(positions,iteration,threadFirstLast,chainDir,chisqDict):
  chisqArray = []
  for i in range(threadFirstLast[0],threadFirstLast[1]):
    #print(positions[i])
    #gm.make_bst(os.path.join(chainDir,beam_bst),positions[i][:nBeamParams])
    #if simulMin == True:
      #gm.make_bst(os.path.join(chainDir,beam_bst),positions[i][:nBeamParams])
      #gm.make_bst(os.path.join(chainDir,target_bst),positions[i][nBeamParams:])
    gm.runGosiaInDir(beamPOINinp,chainDir)
    beamOutputFile = os.path.join(chainDir,gm.getOutputFile(beamPOINinp))
    computedObservables = gm.getPOINobservables(beamOutputFile)
    if simulMin == True:
      gm.runGosiaInDir(targetPOINinp,chainDir)
      targetOutputFile = os.path.join(chainDir,gm.getOutputFile(targetPOINinp))
      computedObservables += gm.getPOINobservables(targetOutputFile)
    expt = []
    for j in range(len(beamExptMap)):
      expt.append(beamExptMap[j][0])
    for j in range(len(targetExptMap)):
      expt.append(targetExptMap[j][0])
    nExpt = max(expt)
    scalingFactors = [0]*nExpt
    dTheta = [10.247,8.317,6.626,5.271,1.0,1.0,1.0,13.23,9.92,7.31,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0]
    #if simulMin == False:
      #for j in range(nExpt):
        #for k in range(len(expt)):
          #if expt[k] == j+1:
            #computedObservables[k] *= normalizingFactors[j]**2
    for j in range(nExpt):
      if simulMin == True:# or normalizingExpts[j] >= 0:
        tempSum1 = 0
        tempSum2 = 0
        for k in range(len(expt)):
          if expt[k] == j+1 and observables[k] != 0:
            #print(expt[k],observables[k],computedObservables[k],uncertainties[k])
            tempSum1 += (observables[k]*computedObservables[k])/uncertainties[k]**2
            tempSum2 += computedObservables[k]**2/uncertainties[k]**2
        scalingFactor = tempSum1/tempSum2
        scalingFactors[j] = scalingFactor
      else:
        tempSum1 = 0
        tempSum2 = 0
        for k in range(len(expt)):
          if normalizingExpts[expt[k]-1] == j+1 and observables[k] != 0 and expt[k] != 0:
            tempSum1 += (observables[k]*computedObservables[k]*normalizingFactors[expt[k]-1])/uncertainties[k]**2
            tempSum2 += (computedObservables[k]*normalizingFactors[expt[k]-1])**2/uncertainties[k]**2
        print(tempSum1,tempSum2)
        if tempSum2 != 0:
          scalingFactor = tempSum1/tempSum2
          for k in range(len(normalizingExpts)):
            if normalizingExpts[k] == j+1:
              scalingFactors[k] = scalingFactor*normalizingFactors[k]
    print(scalingFactors)
    for j in range(nExpt):
      for k in range(len(expt)):
        if expt[k] == j+1:
          computedObservables[k] *= scalingFactors[j]
    nObservables = len(observables)
    chisq = 0
    for j in range(nObservables):
      if observables[j] != 0:
        chisq += ((computedObservables[j]-observables[j])/uncertainties[j])**2
        print(expt[j],computedObservables[j],observables[j],uncertainties[j],chisq)
        #print(expt[j],chisq)
      elif expt[j] != 0:
        if simulMin == True:
          if j < len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1][0]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1][0])/exptUpperLimits[expt[j]-1][0])**2
          elif j >= len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1][1]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1][1])/exptUpperLimits[expt[j]-1][1])**2
        else:
          if j < len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1])/exptUpperLimits[expt[j]-1])**2
            #print(expt[j],computedObservables[j],exptUpperLimits[expt[j]-1],chisq)
            #print(expt[j],chisq)
    chisqArray.append(chisq)
    if simulMin == False:
      a = 1/0
  chisqDict[threadFirstLast[0]] = chisqArray
  return 

#Initializes everything
gm = gosiaManager.gosiaManager(configFile)

#Store important parameters from the config file
simulMin = gm.configDict["simulMin"]
beamINTIinp = gm.configDict["beamINTIinp"]
beamMAPinp = gm.configDict["beamMAPinp"]
beamPOINinp = gm.configDict["beamPOINinp"]
nBeamParams = gm.configDict["nBeamParams"]
beam_bst = gm.configDict["beam_bst"]
nDimensions = nBeamParams
if simulMin == True:
  targetINTIinp = gm.configDict["targetINTIinp"]
  targetMAPinp = gm.configDict["targetMAPinp"]
  targetPOINinp = gm.configDict["targetPOINinp"]
  nTargetParams = gm.configDict["nTargetParams"]
  target_bst = gm.configDict["target_bst"]
  nDimensions += nTargetParams
nThreads = gm.configDict["nThreads"] #number of threads used for particles

#Sets the parameters of the particle swarm to the values I found to be optimal, unless other parameters are specified in the config file.
if "nParticles" in gm.configDict.keys():
  nParticles = int(gm.configDict["nParticles"])
else:
  nParticles = 600
if "cognitiveCoeff" in gm.configDict.keys():
  cognitiveCoeff = float(gm.configDict["cognitiveCoeff"])
else:
  cognitiveCoeff = 1.75
if "socialCoeff" in gm.configDict.keys():
  socialCoeff = float(gm.configDict["socialCoeff"])
else:
  socialCoeff = 1.0
if "inertialCoeff" in gm.configDict.keys():
  inertialCoeff = float(gm.configDict["inertialCoeff"])
else:
  inertialCoeff = 0.7
if "velocityStrategy" in gm.configDict.keys():
  velocityHandler = velocityHandler(strategy=gm.configDict["velocityStrategy"])
else:
  velocityHandler = VelocityHandler(strategy='invert')
if "boundaryStrategy" in gm.configDict.keys():
  boundaryHandler = boundaryHandler(strategy=gm.configDict["boundaryStrategy"])
else:
  boundaryHandler = BoundaryHandler(strategy='reflective')

#Gets the bounds for each matrix element from the INTI files.
beamMatrixElements = gm.getMatrixElements(beamPOINinp)
beamLoBounds = beamMatrixElements["LoBound"].to_numpy()
beamHiBounds = beamMatrixElements["HiBound"].to_numpy()
if simulMin == True:
  targetMatrixElements = gm.getMatrixElements(targetPOINinp)
  targetLoBounds = targetMatrixElements["LoBound"].to_numpy()
  targetHiBounds = targetMatrixElements["HiBound"].to_numpy()
  loBounds = np.concatenate((beamLoBounds,targetLoBounds))
  hiBounds = np.concatenate((beamHiBounds,targetHiBounds))
else:
  loBounds = beamLoBounds
  hiBounds = beamHiBounds
paramBounds = (loBounds,hiBounds)

#Run GOSIA to initialize everything. This is the only time INTI will be run, so use a
#reasonable initial guess. If you don't have one, you can always run with a bad one and 
#use this program to try to get a better initial guess. 
#gm.runGosia2(beamINTIinp)
#gm.runGosia2(targetINTIinp)
#gm.runGosia(beamMAPinp)
#gm.runGosia(targetMAPinp)
beamCorr = gm.getCorrFile(beamINTIinp)
if(simulMin == True):
  targetCorr = gm.getCorrFile(targetINTIinp)
else:
  normalizingExpts,normalizingFactors = gm.getScalingFactors()
  gm.runGosia(beamPOINinp)
  beamPOINout = gm.getOutputFile(beamPOINinp)
  dsig = gm.getDsig(beamPOINout)
  averageAngle = gm.getAverageAngle()
  for j in range(len(normalizingFactors)):
    normalizingFactors[j] /= (dsig[j]*np.sin(averageAngle[j]*np.pi/180.))

#Get experimental observables and the beam and target maps
observables,uncertainties,beamExptMap,targetExptMap = gm.getExperimentalObservables()
#observables[0:288] = rawYields
#uncertainties[0:288] = rawUncertainties
exptUpperLimits = gm.getUpperLimits(beamExptMap,targetExptMap,observables)
#Initialize the particle swarm
#Papers I read suggested that VonNeumann is typically the best topology. With this many 
#dimensions, Star is equivalent to VonNeumann. I found the best approach was to start with a
#local topology (Ring) and increase the number of neighbors every 50 iterations until
#eventually progressing to the Star topology. This allows the particles to first explore the
#space near where they start and prevents premature convergence to a local minimum.
my_topology = Ring(static=False) 

#c1 is the cognitive parameter (velocity component towards particle's own best position)
#c2 is the social parameter (velocity component towards best position of particle it can see)
#c2 pulls towards global minimum in star topology since the graph is fully connected
#w is the inertial parameter (velocity component in direction of last iteration's velocity)
my_options = {'c1' : cognitiveCoeff, 'c2' : socialCoeff, 'w' : inertialCoeff}
my_swarm = P.create_swarm(n_particles=nParticles,dimensions=nDimensions,bounds=paramBounds,options=my_options)

iterSwitch = 350
neighborsArray = [20,30,40,60,80,120,160]

if os.path.exists("checkpoint_%i" % batchNumber):
  f = open("checkpoint_%i/psoSaveIterAndCost.csv" % batchNumber)
  iterStart = int(f.readline().strip()) + 1
  if iterStart > iterSwitch:
    my_topology = Star()
  my_swarm.best_cost = float(f.readline().strip())
  f.close()
  my_swarm.position = np.loadtxt('checkpoint_%i/psoSavePositions.csv' % batchNumber,delimiter=',')
  my_swarm.velocity = np.loadtxt('checkpoint_%i/psoSaveVelocities.csv' % batchNumber,delimiter=',')
  my_swarm.pbest_pos = np.loadtxt('checkpoint_%i/psoSavePBestPos.csv' % batchNumber,delimiter=',')
  my_swarm.pbest_cost = np.loadtxt('checkpoint_%i/psoSavePBestCost.csv' % batchNumber,delimiter=',')
  my_swarm.best_pos = np.loadtxt('checkpoint_%i/psoSaveBestPos.csv' % batchNumber,delimiter=',')  
else:
  os.mkdir("checkpoint_%i" % batchNumber)
  iterStart = 0
iterations = 500
for i in range(iterStart,iterations):
  if i == iterSwitch:
    my_topology = Star()
  
  #Get the chisq for each particle position
  my_swarm.current_cost = getSwarmChisq(my_swarm.position,i,nThreads)
  #initial best cost and best position for each particle 
  if i == 0:
    my_swarm.pbest_cost = my_swarm.current_cost
    my_swarm.pbest_pos = my_swarm.position
  #If it's not the first iteration, check if this position is better than particle's best
  #and update if it is.
  else:
    for j in range(len(my_swarm.current_cost)):
      if my_swarm.current_cost[j] <= my_swarm.pbest_cost[j]:
        my_swarm.pbest_pos[j] = my_swarm.position[j]
        my_swarm.pbest_cost[j] = my_swarm.current_cost[j]
  
  #Track the global best cost
  if np.min(my_swarm.pbest_cost) < my_swarm.best_cost:
    if i < iterSwitch:
      my_swarm.best_pos, my_swarm.best_cost = my_topology.compute_gbest(my_swarm,2,neighborsArray[int(np.floor(i/50))])
      best_iter = i
    else:
      my_swarm.best_pos, my_swarm.best_cost = my_topology.compute_gbest(my_swarm)
      best_iter = i

  print('Iteration: {} | my_swarm.best_cost: {:.4f}'.format(i+1, my_swarm.best_cost))

  
  #Update velocities and positions for the next iteration
  if i < iterSwitch:
    my_swarm.velocity = my_topology.compute_velocity(my_swarm,clamp=None,vh=velocityHandler,bounds=(loBounds,hiBounds))
    my_swarm.position = my_topology.compute_position(my_swarm,bounds=paramBounds,bh=boundaryHandler)
  else:
    my_swarm.velocity = my_topology.compute_velocity(my_swarm)
    my_swarm.position = my_topology.compute_position(my_swarm,bounds=paramBounds,bh=boundaryHandler)
  if i != iterations-1:
    np.savetxt('checkpoint_%i/psoSavePositions.csv' % batchNumber,my_swarm.position,delimiter=',')
    np.savetxt('checkpoint_%i/psoSaveVelocities.csv' % batchNumber,my_swarm.velocity,delimiter=',')
    np.savetxt('checkpoint_%i/psoSavePBestPos.csv' % batchNumber,my_swarm.pbest_pos,delimiter=',')
    np.savetxt('checkpoint_%i/psoSavePBestCost.csv' % batchNumber,my_swarm.pbest_cost,delimiter=',')
    np.savetxt('checkpoint_%i/psoSaveBestPos.csv' % batchNumber,my_swarm.best_pos,delimiter=',')
    f = open('checkpoint_%i/psoSaveIterAndCost.csv' % batchNumber,'w')
    f.write(str(i) + "\n")
    f.write(str(my_swarm.best_cost) + "\n")
    f.close()

f = open('adaptiveParticleSwarmResult_%i.csv' % batchNumber,'w')
f.write('The best cost found by our swarm is: {:.4f}\n'.format(my_swarm.best_cost))
f.write('The best position found by our swarm is: {}\n'.format(my_swarm.best_pos))
f.write('The number of iterations needed was: {}\n'.format(i))
f.close()

for j in range(batchNumber*nThreads,batchNumber*nThreads+nThreads):
  gm.removeSubDirectories(j)
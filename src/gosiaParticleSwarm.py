"""
This code runs a particle swarm optimization scheme on GOSIA to try to find the best 
minimum. PSO is random and not reproducible. For best results, run multiple instances of this
code and take the best result, then put it into a traditional fitter for fine-tuning. This
code requires GOSIA input files for OP,INTI, OP,MAP, and OP,POIN (for beam and target), with 
several specific modifications to them. The code will draw the boundary conditions from the 
OP,INTI input files (from the bounds you set in OP,EXPT). PSO relies on good selection of
boundary conditions for best results, so try to make them tight where possible (for example,
well known target matrix elements should be constrained to within a few sigma of the
literature value). In the OP,POIN input, comments are needed on the following lines in 
OP,YIEL:

NS1,NS2 - please comment this line with !NS
UPL - please comment this line with !UPL
NBRA - please comment this line with !BR
NL - please comment this line with !LT
NDL - please comment this line with !DL
NAMX - please comment this line with !ME

If you are not doing simultaneous beam/target minimization, you will also need to comment your scaling factors:

YNRM - please comment EACH of these lines with !SCL
You must comment each of those lines even for experiments that have independent scaling.

These comments are used for functions that parse the input file to be able to find your limits
and constraints. Note that this code is not set up to handle multiple YNRM or UPL values, so 
you will need to select one instance to comment and that's what the code will use.

You will also need to generate format your yields file in a very specific way. Every 
transition that will be generated by OP,POIN needs to be included, and for unobserved
transitions record the counts and uncertainty as 0 for both. The transitions also need to be
listed in the same order as they appear in OP,POIN (if you're not sure what that is, just 
run OP,POIN and check the order and then format them to match that). The parser assumes that
user-generated files, including this one, use comma separated values, not space separated.
Files that GOSIA generates it assumes the default format in which those files are generated.

There are also two changes to the code that need to be made for each user. In 
fileManagement.py, in the first line of the createSubDirectories function, you will need to 
specify the path to where the subdirectories should be created. The use of subdirectories is
so we can run multiple threads simultaneously. The other change is in myconfig.py. You will 
need to modify this file to include the names of your input files. These should not be 
absolute paths, just the file names. The path will be determined by the directory the code is
run in.

Before running this code, I recommend that you get a 'reasonable' fit from GOSIA or another
minimizer. This is so the corrected yields are relatively correct. Alternatively, you can
start with a terrible fit and use this code to iterively find better 'starting' positions
for the integration, but this takes a long time to run and it will likely be much faster to
just get a quick fit from GOSIA first.

After that, you SHOULD be able to run without any additional modifications. It's possible that
some GOSIA input files will not be read correctly by the parser if they use very different
options than I did. If that's the case, you may need to modify the code in gosiaManager.py
to get it to read your input file properly.
"""
import numpy as np
import pyswarms.backend as P
import gosiaManager
from pyswarms.backend.topology import Ring
from pyswarms.backend.topology import Star
from pyswarms.backend.handlers import VelocityHandler
from pyswarms.backend.handlers import BoundaryHandler
from bisect import bisect
from mpi4py import MPI
import os
import sys


#Initialize the message passing interface. Rank 0 will act as the director and coordinate between the ranks, while other ranks will be used to divide up evaluating the candidate solutions.
comm = MPI.COMM_WORLD
rank = comm.Get_rank()

#Takes an integer as an argument. Only effects where the name of the output file and the temp 
#directories for running GOSIA by default. Useful for running many instances in parallel.
#Ensure a different batch number is used for each instance when running simultaneously.
configFile = sys.argv[1]
batchNumber = int(sys.argv[2])

#This function is called by rank 0 during each iteration of the optimization. It handles dividing up the particles between the other threads and sending the relevant information to those threads. 
#getSwarmChisq takes three arguments: the particle positions, the terminate flag (which instructs the other threads to shut down when the optimization is complete), and the number of threads to be used. 
#The default number of threads is 21, with one thread serving as the director and the rest evaluating the candidate solutions. Because the first thread is always the director, this program requires at least
#two threads to run. However, it is recommended that more threads are used for decreased computation time. With default settings (and no parallel annealing), this program takes approximately 6 hours to complete
#when using 21 threads. 
def getSwarmChisq(positions,terminate,nThreads=21):
  #Divide up the total number of particles as evenly as possible between the threads. Create a list (threadFirstLast) which stores tuples containing the first and the (non-inclusive) last particle to be evaluated
  #by each third.
  particlesPerThread = int(np.ceil(len(positions)/(nThreads-1)))
  threadFirstLast = []
  folderStart = batchNumber*(nThreads-1)
  for j in range(nThreads-1):
    chainDirPrefix = gm.createSubDirectories(folderStart + j)
    threadFirstLast.append((particlesPerThread*j,min(particlesPerThread*(j+1),len(positions))))
  
  #For each non-director thread, we send four messages containing the particle position array, the flag which tells the threads whether or not to terminate, the threadFirstLast array, and the name of the directory
  #where that thread's GOSIA instance will run.
  for j in range(nThreads-1):
    chainDir = chainDirPrefix + str(folderStart + j)
    comm.send(positions,dest=j+1)
    comm.send(terminate,dest=j+1)
    comm.send(threadFirstLast[j],dest=j+1)
    comm.send(chainDir,dest=j+1)
  
  #For each non-director thread, we expect to receive one message in return, which contains the chisq values for each evaluated candidate solution. The message passing interface will wait until each expected message
  #is received before progressing.
  chisqArray = []
  for j in range(nThreads-1):
    threadChisqArray = comm.recv(source=j+1)
    chisqArray += threadChisqArray
  chisqArray = np.array(chisqArray)
  return chisqArray

#This function is called by the 'worker' ranks (every thread except the rank 0 director) to evaluate the candidate solutions for the subset of particles assigned to that thread. It takes as arguments the positions
#array, the tuple containing the span of particles the thread is responsible for, and the name of the directory where the thread will run GOSIA. It returns an array of chisq values for the evaluated candidate
#solutions, which is then sent back to rank 0 through the message passing interface.
def getParticleChisq(positions,threadFirstLast,chainDir):
  chisqArray = []
  #Loop over the particles managed by this thread
  for i in range(threadFirstLast[0],threadFirstLast[1]):
    #Make a .bst file containing the matrix elements for the beam, and for the target if applicable
    gm.make_bst(os.path.join(chainDir,beam_bst),positions[i][:nBeamParams])
    if simulMin == True:
      gm.make_bst(os.path.join(chainDir,target_bst),positions[i][nBeamParams:])
    #Run GOSIA in the appropriate subdirectory, then get the observables from the output file.
    gm.runGosiaInDir(beamPOINinp,chainDir)
    beamOutputFile = os.path.join(chainDir,gm.configDict["beamPOINout"])
    computedObservables = gm.getPOINobservables(beamOutputFile,beamExptMap,beamDoubletMap,beamMultipletMap)
    #Repeat the previous step for the target, if applicable.
    if simulMin == True:
      gm.runGosiaInDir(targetPOINinp,chainDir)
      targetOutputFile = os.path.join(chainDir,gm.configDict["targetPOINout"])
      computedObservables += gm.getPOINobservables(targetOutputFile,targetExptMap,targetDoubletMap,targetMultipletMap)
    #expt stores the GOSIA experiment number associated with each observable. For literature constraints, the experiment number is set to 0.
    expt = []
    for j in range(len(beamExptMap)):
      expt.append(beamExptMap[j][0])
    for j in range(len(targetExptMap)):
      expt.append(targetExptMap[j][0])
    nExpt = max(expt)
    #Compute the scaling factors. These are computed the same way that GOSIA computes them: by finding the value that minimizes the chi-squared value for the relevant experiment or experiments.
    scalingFactors = [0]*nExpt
    for j in range(nExpt):
      if simulMin == True:
        tempSum1 = 0
        tempSum2 = 0
        for k in range(len(expt)):
          if expt[k] == j+1 and observables[k] != 0:
            tempSum1 += (observables[k]*computedObservables[k])/uncertainties[k]**2
            tempSum2 += computedObservables[k]**2/uncertainties[k]**2
        scalingFactor = tempSum1/tempSum2
        scalingFactors[j] = scalingFactor
      #If doing beam- (or target-) only minimization, we can have coupled experiments. The normalizingExpts array, which is populated during initialization if simulMin is False, tracks the experiment number to which each 
      #experiment is normalized. The normalizingFactors array, populated at the same time as normalizingArray, tracks the normalizing factor associated with each experiment. 
      else:
        tempSum1 = 0
        tempSum2 = 0
        for k in range(len(expt)):
          if normalizingExpts[expt[k]-1] == j+1 and observables[k] != 0 and expt[k] != 0:
            tempSum1 += (observables[k]*computedObservables[k]*normalizingFactors[expt[k]-1])/uncertainties[k]**2
            tempSum2 += (computedObservables[k]**2*normalizingFactors[expt[k]-1]**2)/uncertainties[k]**2
        if tempSum2 != 0:
          scalingFactor = tempSum1/tempSum2
          for k in range(len(normalizingExpts)):
            if normalizingExpts[k] == j+1:
              scalingFactors[k] = scalingFactor*normalizingFactors[k]
    #Multiply the computed observables by the scaling factors
    for j in range(nExpt):
      for k in range(len(expt)):
        if expt[k] == j+1:
          computedObservables[k] *= scalingFactors[j]
    #Compute the chisq value from the computed and experimental observables and the experimental uncertainties
    nObservables = len(observables)
    chisq = 0
    for j in range(nObservables):
      #If the observable is set to 0, it is an unobserved transition and does not contribute to the chisq unless the computed observable exceeds the upper limit for the transition. Upper limits are stored in 
      #exptUpperLimits and set during initialization.
      if observables[j] != 0: 
        chisq += ((computedObservables[j]-observables[j])/uncertainties[j])**2
      elif expt[j] != 0:
        #If we are doing simultaneous minimization, we need to determine if the observable is associated with the beam or the target so we know which upper limit to use. If the computed observable exceeds the upper
        #limit, we add a chisq contribution as though the observable was at the upper limit. 
        if simulMin == True:
          if j < len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1][0]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1][0])/exptUpperLimits[expt[j]-1][0])**2
          elif j >= len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1][1]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1][1])/exptUpperLimits[expt[j]-1][1])**2
        else:
          if j < len(beamExptMap) and computedObservables[j] >= exptUpperLimits[expt[j]-1]:
            chisq += ((computedObservables[j]-exptUpperLimits[expt[j]-1])/exptUpperLimits[expt[j]-1])**2
    chisqArray.append(chisq)
  return chisqArray



#Initializes everything
gm = gosiaManager.gosiaManager(configFile)

#Store important parameters from the config file
simulMin = gm.configDict["simulMin"]
beamPOINinp = gm.configDict["beamPOINinp"]
nBeamParams = gm.configDict["nBeamParams"]
beam_bst = gm.configDict["beam_bst"]
nDimensions = nBeamParams
if simulMin == True:
  targetPOINinp = gm.configDict["targetPOINinp"]
  nTargetParams = gm.configDict["nTargetParams"]
  target_bst = gm.configDict["target_bst"]
  nDimensions += nTargetParams
nThreads = gm.configDict["nThreads"] #number of threads used for particles

#Set the parameters of the particle swarm to the values I found to be optimal, unless other parameters are specified in the config file.
if "nParticles" in gm.configDict.keys():
  nParticles = int(gm.configDict["nParticles"])
else:
  nParticles = 600
if "nIterations" in gm.configDict.keys():
  nIterations = int(gm.configDict["nIterations"])
else:
  nIterations = 500
if "cognitiveCoeff" in gm.configDict.keys():
  cognitiveCoeff = float(gm.configDict["cognitiveCoeff"])
else:
  cognitiveCoeff = 1.75
if "socialCoeff" in gm.configDict.keys():
  socialCoeff = float(gm.configDict["socialCoeff"])
else:
  socialCoeff = 1.0
if "inertialCoeff" in gm.configDict.keys():
  inertialCoeff = float(gm.configDict["inertialCoeff"])
else:
  inertialCoeff = 0.7
if "velocityStrategy" in gm.configDict.keys():
  velocityHandler = velocityHandler(strategy=gm.configDict["velocityStrategy"])
else:
  velocityHandler = VelocityHandler(strategy='invert')
if "boundaryStrategy" in gm.configDict.keys():
  boundaryHandler = boundaryHandler(strategy=gm.configDict["boundaryStrategy"])
else:
  boundaryHandler = BoundaryHandler(strategy='reflective')
if "parallelAnnealing" in gm.configDict.keys():
  parallelAnnealing = gm.configDict["parallelAnnealing"]
else:
  parallelAnnealing = False
if parallelAnnealing == True:
  if "annealingLayers" in gm.configDict.keys():
    annealingLayers = int(gm.configDict["annealingLayers"])
  else:
    annealingLayers = 5
  if "annealingRates" in gm.configDict.keys():
    annealingRates = gm.configDict["annealingRates"].split(",")
    annealingRates = [float(x) for x in annealingRates]
  else:
    annealingRates = [0.3,0.4,0.5,0.6,0.7]
  if "crossoverIterations" in gm.configDict.keys():
    crossoverIterations = gm.configDict["crossoverIterations"].split(",")
    crossoverIterations = [int(x) for x in crossoverIterations]
  else:
    crossoverIterations = [100,200,300,400]
else:
  annealingLayers = 1
  annealingRates = [inertialCoeff]
  crossoverIterations = []
if "topology" in gm.configDict.keys():
  graphTopology = gm.configDict["topology"]
else:
  graphTopology = "star"
#If we are using the dynamic topology, we need to determine when and how the topology will change. This can be user specified or use default values.
if graphTopology == "dynamic":
  #nNeighbors will be an integer or a list of integers
  if "nNeighbors" in gm.configDict.keys():
    neighborsArray = gm.configDict["nNeighbors"].split(",")
    neighborsArray = [int(x) for x in neighborsArray]
  else:
    neighborsArray = [20,30,40,60,80,120,160]
  #topologySwitchIterations will be an integer or a list of integers. If there is only one element in the neighborsArray and this parameter is not specified by the user, we will use the ring topology.
  if "topologySwitchIterations" in gm.configDict.keys():
    topologySwitchIterations = gm.configDict["topologySwitchIterations"].split(",")
    topologySwitchIterations = [int(x) for x in topologySwitchIterations]
  elif len(neighborsArray) == 1:
    topologySwitchIterations = [10*nIterations]
  else:
    topologySwitchIterations = [50,100,150,200,250,300,350]
  #Check if the lengths of neighborsArray and topologySwitchIterations are compatible. They should be either the same length, in which case the final topology will be the star topology, or topologySwitchIterations
  #should have one fewer element, in which case the last specified topology will be the final topology.
  if len(topologySwitchIterations) == len(neighborsArray):
    neighborsArray.append("star")
  elif len(topologySwitchIterations) != (len(neighborsArray)-1):
    raise Exception("ERROR: %i topologies are specified by the parameter nNeighbors, and %i topology switch points are specified by the parameter topologySwitchIterations. The number of topologies must be either equal to the number of switch points or one greater than the number of switch points. If they are equal, the final switch point will switch to the star topology." % (len(neighborsArray),len(topologySwitchIterations)))



#Check if the number of annealing rates and crossover points (if applicable) matches the number of annealing layers. Only rank 0 performs this check to avoid repeat error messages.
if rank == 0:
  if len(annealingRates) != annealingLayers:
    raise Exception("ERROR: Number of annealing layers is set to %i but %i annealing rates are specified." % annealingLayers,len(annealingRates))
  if annealingLayers > 1:
    if len(crossoverIterations) < annealingLayers - 1:
      print("WARNING: Number of crossovers is not sufficient to pass all particles through all annealing layers.")
    elif len(crossoverIterations) > annealingLayers - 1:
      print("WARNING: Number of crossovers is greater than the number of annealing layers - 1. This means that some particle swarms will pass through the same layer more than once.")


#Gets the bounds for each matrix element from the INTI files.
beamMatrixElements = gm.getMatrixElements(beamPOINinp)
beamLoBounds = beamMatrixElements["LoBound"].to_numpy()
beamHiBounds = beamMatrixElements["HiBound"].to_numpy()
if simulMin == True:
  targetMatrixElements = gm.getMatrixElements(targetPOINinp)
  targetLoBounds = targetMatrixElements["LoBound"].to_numpy()
  targetHiBounds = targetMatrixElements["HiBound"].to_numpy()
  loBounds = np.concatenate((beamLoBounds,targetLoBounds))
  hiBounds = np.concatenate((beamHiBounds,targetHiBounds))
else:
  loBounds = beamLoBounds
  hiBounds = beamHiBounds
paramBounds = (loBounds,hiBounds)

#If simulMin is false, we will need some additional information. The getScalingFactors function parses the GOSIA (beam) input file provided to determine which experiments are coupled to one another and what the
#scaling factors are. This information is stored in the normalizingExpts and normalizingFactors arrays. The normalizing factors need to be scaled to match the computation done in GOSIA, and this requires the DSIG
#parameter from GOSIA. GOFISH computes DSIG in the same way that GOSIA does, using the getDsig function. Due to the lack of comments in the GOSIA code, I do not fully understand what is being calculated. Thank you 
#to Dr. Daniel Rhodes for figuring out how DSIG is calculated so that the calculation could be replicated here. 
if(simulMin == False):
  normalizingExpts,normalizingFactors = gm.getScalingFactors()
  #dsig = gm.getDsig(gm.configDict["beamPOINout"])
  dsig = gm.getDsig()
  averageAngle = gm.getAverageAngle()
  for j in range(len(normalizingFactors)):
    normalizingFactors[j] /= (dsig[j]*np.sin(averageAngle[j]*np.pi/180.))

#Get experimental observables and the beam and target maps
observables,uncertainties,beamExptMap,targetExptMap,beamDoubletMap,targetDoubletMap,beamMultipletMap,targetMultipletMap = gm.getExperimentalObservables()
exptUpperLimits = gm.getUpperLimits(beamExptMap,targetExptMap,observables)

if rank == 0:
  #This program can handle two topology options, Dynamic or Star. The Star topology is the fully connected 'gbest' toplogy, and the 
  #dynamic toplogy begins as a ring topology with n=20 and slowly adds connections until the graph is fully connected. The Star 
  #topology was observed to converge faster. In theory, the advantage of lbest topologies such as the ring topology is that they are 
  #less likely to prematurely converge to a local minimum. In the analysis of the Sn CoulEx on which this data was benchmarked, >90%
  #of runs with the star topology converged to the apparent global minimum, so I used the star topology. However, the dynamic topology
  #remains included for use by future users. 
  
  if graphTopology == "dynamic":
    my_topology = Ring(static=False)
  elif graphTopology == "star":
    my_topology = Star()
  else:
    raise Exception("ERROR: Topology %s is not available. The options for toplogy are star or dynamic." % graphTopology)

  #c1 is the cognitive parameter (velocity component towards particle's own best position)
  #c2 is the social parameter (velocity component towards best position of particle it can communicate with)
  #c2 pulls towards global minimum in star topology since the graph is fully connected
  #w is the inertial parameter (velocity component in direction of last iteration's velocity)
  #The my_swarms array stores the particle swarm objects. If parallel annealing is not enabled, this 
  #will be an array of length 1. 
  my_swarms = []
  for i in range(annealingLayers):
    my_options = {'c1' : cognitiveCoeff, 'c2' : socialCoeff, 'w' : annealingRates[i]}
    my_swarms.append(P.create_swarm(n_particles=nParticles,dimensions=nDimensions,bounds=paramBounds,options=my_options))
    
  

  #We use a checkpointing system that saves the state of the graph at each iteration so that
  #jobs can be run on a scavenger queue or otherwise interupted and resumed. 
  if os.path.exists("checkpoint_%i/psoSaveIterAndCost_0.csv" % batchNumber):
    #We save a separate set of checkpoint files for each annealing layer, so we loop over them here.
    for j in range(annealingLayers):
      f = open("checkpoint_%i/psoSaveIterAndCost_%i.csv" % (batchNumber,j))
      #Get the current iteration number and set the toplogy accordingly.
      if j == 0:
        iterStart = int(f.readline().strip()) + 1
        if graphTopology == "dynamic":
          currentTopologyIteration = bisect(topologySwitchIterations,i)
          if neighborsArray[currentTopologyIteration] == "star":
            graphTopology = "star"
            my_topology = Star()
          else:
            nNeighbors = neighborsArray[currentTopologyIteration]
      my_swarms[j].best_cost = float(f.readline().strip())
      f.close()
      #Load the parameters for this swarm
      my_swarms[j].position = np.loadtxt('checkpoint_%i/psoSavePositions_%i.csv' % (batchNumber,j),delimiter=',')
      my_swarms[j].velocity = np.loadtxt('checkpoint_%i/psoSaveVelocities_%i.csv' % (batchNumber,j),delimiter=',')
      my_swarms[j].pbest_pos = np.loadtxt('checkpoint_%i/psoSavePBestPos_%i.csv' % (batchNumber,j),delimiter=',')
      my_swarms[j].pbest_cost = np.loadtxt('checkpoint_%i/psoSavePBestCost_%i.csv' % (batchNumber,j),delimiter=',')
      my_swarms[j].best_pos = np.loadtxt('checkpoint_%i/psoSaveBestPos_%i.csv' % (batchNumber,j),delimiter=',')  
  #If we are not resuming a partially completed run and the checkpoint directory does not exist, create it here and initalize at iteration 0.
  else:
    if not os.path.exists("checkpoint_%i" % batchNumber):
      os.mkdir("checkpoint_%i" % batchNumber)
    iterStart = 0
  
  #Now we start the main part of of code: the for loop over the optimizer iterations.
  for i in range(iterStart,nIterations):
    #If we are using the dynamic topology, check what the current topology should be
    if graphTopology == "dynamic":
      currentTopologyIteration = bisect(topologySwitchIterations,i)
      if neighborsArray[currentTopologyIteration] == "star":
        graphTopology = "star"
        my_topology = Star()
      else:
        nNeighbors = neighborsArray[currentTopologyIteration]


    #If we have multiple annealing layers, check if we are at a crossover iteration. If we are, cycle the annealing layers.
    if annealingLayers > 1:
      if i in crossoverIterations:
        tmp_swarms = my_swarms
        for k in range(annealingLayers-1):
          my_swarms[k+1] = tmp_swarms[k]
          my_swarms[k].options = {'c1' : cognitiveCoeff, 'c2' : socialCoeff, 'w' : annealingRates[k]}
        my_swarms[0] = tmp_swarms[-1]
        my_swarms[0].options = {'c1' : cognitiveCoeff, 'c2' : socialCoeff, 'w' : annealingRates[0]}
    
    #Loop over our annealing layers
    for k in range(annealingLayers):
      #If we are on the final iteration and the final annealing layer, we set the terminate flag to "True" so the other ranks terminate after the final calculation.
      if (i == nIterations - 1) and (k == annealingLayers - 1):
        terminate = True
      else:
        terminate = False
      #Evaluate the candidate solutions for each particle.
      my_swarms[k].current_cost = getSwarmChisq(my_swarms[k].position,terminate,nThreads)
    #initial best cost and best position for each particle 
      if i == 0:
        my_swarms[k].pbest_cost = my_swarms[k].current_cost
        my_swarms[k].pbest_pos = my_swarms[k].position
    #If it's not the first iteration, check if this position is better than particle's best
    #and update if it is.
      else:
        for j in range(len(my_swarms[k].current_cost)):
          if my_swarms[k].current_cost[j] <= my_swarms[k].pbest_cost[j]:
            my_swarms[k].pbest_pos[j] = my_swarms[k].position[j]
            my_swarms[k].pbest_cost[j] = my_swarms[k].current_cost[j]
    #Track the global best cost
    if graphTopology == "dynamic":
      if np.min(my_swarms[k].pbest_cost) < np.min(my_swarms[k].best_cost):
          my_swarms[k].best_pos, my_swarms[k].best_cost = my_topology.compute_gbest(my_swarms[k],2,nNeighbors)
    else:
      if np.min(my_swarms[k].pbest_cost) < my_swarms[k].best_cost:
          my_swarms[k].best_pos, my_swarms[k].best_cost = my_topology.compute_gbest(my_swarms[k])
    #print('Iteration: {} | my_swarm.best_cost: {:.4f}'.format(i+1, my_swarm.best_cost))

    for k in range(annealingLayers):
    #Update velocities and positions for the next iteration
      if graphTopology == "dynamic":
        my_swarms[k].velocity = my_topology.compute_velocity(my_swarms[k],clamp=None,vh=velocityHandler,bounds=(loBounds,hiBounds))
        my_swarms[k].position = my_topology.compute_position(my_swarms[k],bounds=paramBounds,bh=boundaryHandler)
      else:
        my_swarms[k].velocity = my_topology.compute_velocity(my_swarms[k])
        my_swarms[k].position = my_topology.compute_position(my_swarms[k],bounds=paramBounds,bh=boundaryHandler)
      #Save checkpoint information in case this run gets interupted.
      f = open('checkpoint_%i/psoBestCostTracker_%i.csv' % (batchNumber,k),'a')
      f.write(str(i) + "," + str(my_swarms[k].best_cost) + "\n")
      f.close()
      if i != nIterations-1:
        np.savetxt('checkpoint_%i/psoSavePositions_%i.csv' % (batchNumber,k),my_swarms[k].position,delimiter=',')
        np.savetxt('checkpoint_%i/psoSaveVelocities_%i.csv' % (batchNumber,k),my_swarms[k].velocity,delimiter=',')
        np.savetxt('checkpoint_%i/psoSavePBestPos_%i.csv' % (batchNumber,k),my_swarms[k].pbest_pos,delimiter=',')
        np.savetxt('checkpoint_%i/psoSavePBestCost_%i.csv' % (batchNumber,k),my_swarms[k].pbest_cost,delimiter=',')
        np.savetxt('checkpoint_%i/psoSaveBestPos_%i.csv' % (batchNumber,k),my_swarms[k].best_pos,delimiter=',')
        f = open('checkpoint_%i/psoSaveIterAndCost_%i.csv' % (batchNumber,k),'w')
        f.write(str(i) + "\n")
        f.write(str(my_swarms[k].best_cost) + "\n")
        f.close()

  #After the optimization is complete, find the best result and save it to file.
  bestSwarm = 0
  bestCost = my_swarms[0].best_cost
  if annealingLayers > 1:
    for k in range(1,annealingLayers):
      if my_swarms[k].best_cost < bestCost:
        bestCost = my_swarms[k].best_cost
        bestSwarm = k
  f = open('particleSwarmResult_%i.csv' % batchNumber,'w')
  f.write('The best cost found by our swarm is: {:.4f}\n'.format(bestCost))
  if graphTopology == "dynamic":
    bestPosIndex = np.where(my_swarms[k].pbest_cost == bestCost)
    globalBestPos = my_swarms[k].pbest_pos[bestPosIndex]
    f.write('The best position found by our swarm is: {}\n'.format(globalBestPos))
  else:
    f.write('The best position found by our swarm is: {}\n'.format(my_swarms[bestSwarm].best_pos))
  f.close()

  #Clean up the subdirectories made for running GOSIA and terminate the program.
  for j in range(batchNumber*(nThreads-1),batchNumber*(nThreads-1)+nThreads-1):
    gm.removeSubDirectories(j)
  print("Finalizing Rank 0")
  MPI.Finalize()

#This block of code is for the non-zero ranks. Once they have been initialized (above) they remain in this loop until the terminate signal is received.
elif rank > 0:
  terminate = False
  while terminate == False:
    #We expect to receive the positions, the terminate flag, the particle numbers assigned to this rank, and the subdirectory name as messages passed from rank 0.
    positions = comm.recv(source=0)
    terminate = comm.recv(source=0)
    threadFirstLast = comm.recv(source=0)
    chainDir = comm.recv(source=0)
    #Evaluate th candidate solutions and send the chisq values back to rank 0.
    chisqArray = getParticleChisq(positions,threadFirstLast,chainDir)
    comm.send(chisqArray,dest=0)
  print("Finalizing Rank %i" % rank)
  MPI.Finalize()

